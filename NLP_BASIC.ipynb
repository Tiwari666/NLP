{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj5PxUvx+8FNgnxb4ZMsij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tiwari666/NLP/blob/main/NLP_BASIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top 14 NLTK preprocessing steps\n"
      ],
      "metadata": {
        "id": "NmS2WfZI9hCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Tokenization"
      ],
      "metadata": {
        "id": "rb4Glw4XE4Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvTJkUYSFArU",
        "outputId": "b8d03664-d05d-4c3d-933f-f786a128d0a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'quick', 'brown', 'fox', 'is', 'jumping', 'over', 'the', 'lazy', 'dog', ',', 'which', 'is', 'located', 'at', 'http', ':', '//example.com', ',', 'but', 'it', 'did', \"n't\", 'catch', 'the', 'fish', 'because', 'it', 'was', 'too', 'quick', 'and', 'the', 'lazy', 'dog', 'did', \"n't\", 'move', 'quickly', 'enough', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Lowercasing:\n",
        "Converte all text to lowercase, making it case-insensitive."
      ],
      "metadata": {
        "id": "uDK9_TDFFD9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# lowercase the tokens\n",
        "lowercased_tokens = [token.lower() for token in tokens]\n",
        "\n",
        "print(\"Lowercased tokens:\", lowercased_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF0saTg7FtdS",
        "outputId": "c3d53ced-5072-444b-96e4-3b88595e3eff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased tokens: ['the', 'quick', 'brown', 'fox', 'is', 'jumping', 'over', 'the', 'lazy', 'dog', ',', 'which', 'is', 'located', 'at', 'http', ':', '//example.com', ',', 'but', 'it', 'did', \"n't\", 'catch', 'the', 'fish', 'because', 'it', 'was', 'too', 'quick', 'and', 'the', 'lazy', 'dog', 'did', \"n't\", 'move', 'quickly', 'enough', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Remove punctuation\n",
        "Removing punctuation marks simplifies the text and make it easier to process."
      ],
      "metadata": {
        "id": "ldGHBaxAF3rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# remove punctuation\n",
        "filtered_tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "print(\"Tokens without punctuation:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_wgKy2XGAHq",
        "outputId": "7e66c2af-2047-43e0-c481-f3170972ea20"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without punctuation: ['The', 'quick', 'brown', 'fox', 'is', 'jumping', 'over', 'the', 'lazy', 'dog', 'which', 'is', 'located', 'at', 'http', '//example.com', 'but', 'it', 'did', \"n't\", 'catch', 'the', 'fish', 'because', 'it', 'was', 'too', 'quick', 'and', 'the', 'lazy', 'dog', 'did', \"n't\", 'move', 'quickly', 'enough']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Remove stop words\n",
        "Removing common words that do not add significant meaning to the text, such as “a,” “an,” and “the.”\n",
        "\n",
        "To remove common stop words from a list of tokens using NLTK, one can use the nltk.corpus.stopwords.words() function to get a list of stopwords in a specific language and filter the tokens using this list. Example:"
      ],
      "metadata": {
        "id": "p6pk_s7bGG3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# get list of stopwords in English\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "# remove stopwords\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "\n",
        "print(\"Tokens without stopwords:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xILKEjuGP9p",
        "outputId": "1e546812-c429-4368-83ad-1cd5a36a0eda"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without stopwords: ['quick', 'brown', 'fox', 'jumping', 'lazy', 'dog', ',', 'located', 'http', ':', '//example.com', ',', \"n't\", 'catch', 'fish', 'quick', 'lazy', 'dog', \"n't\", 'move', 'quickly', 'enough', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Remove extra whitespace:\n",
        "\n",
        "To remove extra white space within a text string with NLTK, one may employ string.strip() to eliminate leading and trailing whitespace, while string.replace() can be utilized to substitute multiple consecutive whitespace characters with a single space."
      ],
      "metadata": {
        "id": "TYUbbrarG74F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "# input text with extra white space\n",
        "text = \"   The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.   \"\n",
        "\n",
        "# remove leading and trailing white space\n",
        "text = text.strip()\n",
        "\n",
        "# replace multiple consecutive white space characters with a single space\n",
        "text = \" \".join(text.split())\n",
        "\n",
        "print(\"Cleaned text:\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Bd8n1QpHt3M",
        "outputId": "e2e18469-86da-4333-ee46-c276be49ef55"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text: The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Remove URLs\n",
        "To remove URLs from a string of text using NLTK, one can use a regular expression pattern to identify URLs and replace them with an empty string. Here is an example of how to do this:"
      ],
      "metadata": {
        "id": "zRSRqVoCH3Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "# input text with URLs\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# define a regular expression pattern to match URLs\n",
        "pattern = r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
        "\n",
        "# replace URLs with an empty string\n",
        "cleaned_text = re.sub(pattern, \"\", text)\n",
        "\n",
        "print(\"Text without URLs:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnsMftsdIEUx",
        "outputId": "c28fb285-011d-4522-bfae-7b5aecfa8b39"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without URLs: The quick brown fox is jumping over the lazy dog, which is located at , but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Remove HTML code:\n",
        "To remove HTML code from a string of text using NLTK, one can use a regular expression pattern to identify HTML tags and replace them with an empty string."
      ],
      "metadata": {
        "id": "ytBCShV2IKZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "# input text with HTML code\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# define a regular expression pattern to match HTML tags\n",
        "pattern = r\"<[^>]+>\"\n",
        "\n",
        "# replace HTML tags with an empty string\n",
        "cleaned_text = re.sub(pattern, \"\", text)\n",
        "\n",
        "print(\"Text without HTML code:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ry052HrIq7I",
        "outputId": "d30aed5f-66be-4712-91b5-f9b7521df96a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without HTML code: The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Remove frequent words:\n",
        "\n",
        "To remove frequent words (also known as “high-frequency words”) from a list of tokens using NLTK, one can use the nltk.FreqDist() function to calculate the frequency of each word and filter out the most common ones."
      ],
      "metadata": {
        "id": "VIePjj9dIslH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# calculate the frequency of each word\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "\n",
        "# remove the most common words (e.g., the top 10% of words by frequency)\n",
        "filtered_tokens = [token for token in tokens if fdist[token] < fdist.N() * 0.1]\n",
        "\n",
        "print(\"Tokens without frequent words:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LfMt09MI4n4",
        "outputId": "1fa4edfc-1041-4067-91fc-8de383042e70"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without frequent words: ['The', 'quick', 'brown', 'fox', 'is', 'jumping', 'over', 'the', 'lazy', 'dog', ',', 'which', 'is', 'located', 'at', 'http', ':', '//example.com', ',', 'but', 'it', 'did', \"n't\", 'catch', 'the', 'fish', 'because', 'it', 'was', 'too', 'quick', 'and', 'the', 'lazy', 'dog', 'did', \"n't\", 'move', 'quickly', 'enough', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9)  Spelling correction\n",
        "Correcting misspelt words is sometimes important so that the meaning of a sentence can be interpreted later in the processing.\n",
        "\n",
        "To perform spelling correction on a list of tokens using NLTK, one can use the nltk.corpus.words.words() function to get a list of English words and the nltk.edit_distance() function to calculate the edit distance between a word and the words in the list."
      ],
      "metadata": {
        "id": "bCZJlTRKJMem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "\n",
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish beacuse it was too quiick and the lazy dog didn't move quickiy enough.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# get list of English words\n",
        "words = nltk.corpus.words.words()\n",
        "\n",
        "# correct spelling of each word\n",
        "corrected_tokens = []\n",
        "for token in tokens:\n",
        "    # find the word with the lowest edit distance\n",
        "    corrected_token = min(words, key=lambda x: nltk.edit_distance(x, token))\n",
        "    corrected_tokens.append(corrected_token)\n",
        "\n",
        "print(\"Corrected tokens:\", corrected_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmpftdXlJcil",
        "outputId": "1c8336ff-c04a-409f-f771-1ada48bf8157"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected tokens: ['The', 'quick', 'brown', 'fox', 'is', 'bumping', 'over', 'the', 'lazy', 'dog', 'A', 'which', 'is', 'lobated', 'at', 'atap', 'A', 'example', 'A', 'but', 'it', 'did', 'nat', 'catch', 'the', 'fish', 'Aeacus', 'it', 'was', 'too', 'quick', 'and', 'the', 'lazy', 'dog', 'did', 'nat', 'move', 'quickie', 'enough', 'A']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Stemming\n",
        "Reducing words to their base form, such as converting “jumping” to “jump.”\n",
        "\n",
        "To perform stemming on a list of tokens using NLTK, one can use the nltk.stem.PorterStemmer() function to create a stemmer object and the stem() method to stem each token."
      ],
      "metadata": {
        "id": "TH7BxT_lKSIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# create stemmer object\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# stem each token\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUXCdpz8JjL0",
        "outputId": "c7040ecc-0c30-4897-e86f-53d801f31791"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed tokens: ['the', 'quick', 'brown', 'fox', 'is', 'jump', 'over', 'the', 'lazi', 'dog', ',', 'which', 'is', 'locat', 'at', 'http', ':', '//example.com', ',', 'but', 'it', 'did', \"n't\", 'catch', 'the', 'fish', 'becaus', 'it', 'wa', 'too', 'quick', 'and', 'the', 'lazi', 'dog', 'did', \"n't\", 'move', 'quickli', 'enough', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Lemmatization\n",
        "A more complicated and accurate method of reducing words to their base form than stemming.\n",
        "\n",
        "To perform lemmatization on a token list using NLTK, one can utilize the nltk.stem.WordNetLemmatizer() function to instantiate a lemmatizer object, followed by applying the lemmatize() method to each token for lemmatization.\n",
        "\n",
        "\n",
        "The WordNet lemmatizer uses the WordNet database of English words to lemmatize the tokens, taking into account the part of speech and the context in which the word is used. One can specify the part of speech of the token using the pos argument of the lemmatize() method (e.g., \"v\" for verbs, etc, \"n\" for nouns, etc.)."
      ],
      "metadata": {
        "id": "WAXcPy5mKyai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# create lemmatizer object\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# lemmatize each token\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "print(\"Lemmatized tokens:\", lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "namwK0g3K0lH",
        "outputId": "007b395d-4f56-435e-d888-d03c1f6f0bdc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deal', 'with', 'the', 'interaction', 'between', 'computer', 'and', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# create stemmer object\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# stem each token\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79db3a65-8d4f-4974-bb79-b2c20dd7dc7b",
        "id": "CmFu3X_r13io"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed tokens: ['the', 'quick', 'brown', 'fox', 'is', 'jump', 'over', 'the', 'lazi', 'dog', ',', 'which', 'is', 'locat', 'at', 'http', ':', '//example.com', ',', 'but', 'it', 'did', \"n't\", 'catch', 'the', 'fish', 'becaus', 'it', 'wa', 'too', 'quick', 'and', 'the', 'lazi', 'dog', 'did', \"n't\", 'move', 'quickli', 'enough', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. Part-of-speech tagging\n",
        "Identifying the part of speech of each word in the text, such as noun, verb, or adjective.\n",
        "\n",
        "To perform part of speech (POS) tagging on a list of tokens using NLTK, one can use the nltk.pos_tag() function to tag the tokens with their corresponding POS tags."
      ],
      "metadata": {
        "id": "aMRNT2G2LGE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# tag the tokens with their POS tags\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"Tagged tokens:\", tagged_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9_18NtmLNCN",
        "outputId": "533ee1df-5b31-4c3f-a6af-3da7ac9f1c92"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged tokens: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'IN'), ('deals', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('(', '('), ('natural', 'JJ'), (')', ')'), ('language', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# create stemmer object\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# stem each token\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4cd537a-b8b0-41cd-dff8-8ab79edbed81",
        "id": "bD44u0372IHP"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed tokens: ['the', 'quick', 'brown', 'fox', 'is', 'jump', 'over', 'the', 'lazi', 'dog', ',', 'which', 'is', 'locat', 'at', 'http', ':', '//example.com', ',', 'but', 'it', 'did', \"n't\", 'catch', 'the', 'fish', 'becaus', 'it', 'wa', 'too', 'quick', 'and', 'the', 'lazi', 'dog', 'did', \"n't\", 'move', 'quickli', 'enough', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. Named Entity Recognition\n",
        "Extracting named entities from a text, like a person’s name.\n",
        "\n",
        "See also  Link Prediction For Graph Neural Networks (GNN) Made Simple & 6 Powerful Tools\n",
        "To perform named entity recognition (NER) on a list of tokens using NLTK, one can use the nltk.ne_chunk() function to identify and label named entities in the tokens."
      ],
      "metadata": {
        "id": "5_u9mSbbLT7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough. Kyal Smith works at Facebook in New York.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# tag the tokens with their part of speech\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "# identify named entities\n",
        "named_entities = nltk.ne_chunk(tagged_tokens)\n",
        "\n",
        "print(\"Named entities:\", named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alXMQo8ZLfrf",
        "outputId": "cf743345-c6b6-4c4e-d580-81ba169b0dd5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities: (S\n",
            "  The/DT\n",
            "  quick/JJ\n",
            "  brown/NN\n",
            "  fox/NN\n",
            "  is/VBZ\n",
            "  jumping/VBG\n",
            "  over/IN\n",
            "  the/DT\n",
            "  lazy/JJ\n",
            "  dog/NN\n",
            "  ,/,\n",
            "  which/WDT\n",
            "  is/VBZ\n",
            "  located/VBN\n",
            "  at/IN\n",
            "  http/NN\n",
            "  :/:\n",
            "  //example.com/NN\n",
            "  ,/,\n",
            "  but/CC\n",
            "  it/PRP\n",
            "  did/VBD\n",
            "  n't/RB\n",
            "  catch/VB\n",
            "  the/DT\n",
            "  fish/NN\n",
            "  because/IN\n",
            "  it/PRP\n",
            "  was/VBD\n",
            "  too/RB\n",
            "  quick/JJ\n",
            "  and/CC\n",
            "  the/DT\n",
            "  lazy/JJ\n",
            "  dog/NN\n",
            "  did/VBD\n",
            "  n't/RB\n",
            "  move/VB\n",
            "  quickly/RB\n",
            "  enough/RB\n",
            "  ./.\n",
            "  (PERSON Kyal/NNP Smith/NNP)\n",
            "  works/VBZ\n",
            "  at/IN\n",
            "  (ORGANIZATION Facebook/NNP)\n",
            "  in/IN\n",
            "  (GPE New/NNP York/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. Normalization\n",
        "Standardising words or phrases that have multiple possible forms or spellings (e.g. “American” and “US” could both be normalised to “United States”). This can be easily done with a list of synonyms or industry-specific terms."
      ],
      "metadata": {
        "id": "nqIcm3wKMKAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLTK preprocessing example code for Sentiment Analysis:"
      ],
      "metadata": {
        "id": "gegtdQejMiez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "g53Txy7jTihy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1hCqXA7Ti1A",
        "outputId": "e6a1e3a9-bcd6-4014-dda4-97899eadaaca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8XRaePUp1LG",
        "outputId": "0ac3cd1b-050c-4103-c42b-3cbf824a907d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "RG3ciOr9p6td"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "from nltk import ne_chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WEj8fXKqLvW",
        "outputId": "716ac0f1-a83c-4eba-fc39-d7c65a98e037"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n",
        "from nltk.corpus import words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntu7Pv6VqhgF",
        "outputId": "b6059e80-6265-4701-e9a4-c2308000dca1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGDnvzMUqtf8",
        "outputId": "7e9a1740-8b61-4bd7-fb26-97e068e052be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UTuZshnrFNl",
        "outputId": "39fce9f2-37b0-427f-e42e-ffd285347e1b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "text = \"The quick brown fox is jumping over the lazy dog, which is located at http://example.com, but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\"\n",
        "\n",
        "# tokenization\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# part-of-speech tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(\"POS tags:\", pos_tags)\n",
        "\n",
        "# named entity recognition\n",
        "named_entities = nltk.ne_chunk(pos_tags)\n",
        "print(\"Named entities:\", named_entities)\n",
        "\n",
        "# lemmatization\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmas:\", lemmas)\n",
        "\n",
        "# stopword removal\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "print(\"Filtered tokens:\", filtered_tokens)\n",
        "\n",
        "# URL Removal --define a regular expression pattern to match URLs\n",
        "pattern = r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
        "\n",
        "# replace URLs with an empty string\n",
        "cleaned_text = re.sub(pattern, \"\", text)\n",
        "\n",
        "print(\"Text without URLs:\", cleaned_text)\n",
        "\n",
        "# text classification (example using a simple Naive Bayes classifier)\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "# training data (using a toy dataset for illustration purposes)\n",
        "training_data = [(\"It was a great movie.\", \"pos\"), (\"I hated the book.\", \"neg\"), (\"The book was okay.\", \"pos\")]\n",
        "\n",
        "# extract features from the training data\n",
        "def extract_features(text):\n",
        "    features = {}\n",
        "    for word in nltk.word_tokenize(text):\n",
        "        features[word] = True\n",
        "    return features\n",
        "\n",
        "# create a list of feature sets and labels\n",
        "feature_sets = [(extract_features(text), label) for (text, label) in training_data]\n",
        "\n",
        "# train the classifier\n",
        "classifier = NaiveBayesClassifier.train(feature_sets)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLwQV0y0M2-j",
        "outputId": "3172f4f3-993c-465b-bb01-a3086c29bb34"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'quick', 'brown', 'fox', 'is', 'jumping', 'over', 'the', 'lazy', 'dog', ',', 'which', 'is', 'located', 'at', 'http', ':', '//example.com', ',', 'but', 'it', 'did', \"n't\", 'catch', 'the', 'fish', 'because', 'it', 'was', 'too', 'quick', 'and', 'the', 'lazy', 'dog', 'did', \"n't\", 'move', 'quickly', 'enough', '.']\n",
            "POS tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), (',', ','), ('which', 'WDT'), ('is', 'VBZ'), ('located', 'VBN'), ('at', 'IN'), ('http', 'NN'), (':', ':'), ('//example.com', 'NN'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('catch', 'VB'), ('the', 'DT'), ('fish', 'NN'), ('because', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('too', 'RB'), ('quick', 'JJ'), ('and', 'CC'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('did', 'VBD'), (\"n't\", 'RB'), ('move', 'VB'), ('quickly', 'RB'), ('enough', 'RB'), ('.', '.')]\n",
            "Named entities: (S\n",
            "  The/DT\n",
            "  quick/JJ\n",
            "  brown/NN\n",
            "  fox/NN\n",
            "  is/VBZ\n",
            "  jumping/VBG\n",
            "  over/IN\n",
            "  the/DT\n",
            "  lazy/JJ\n",
            "  dog/NN\n",
            "  ,/,\n",
            "  which/WDT\n",
            "  is/VBZ\n",
            "  located/VBN\n",
            "  at/IN\n",
            "  http/NN\n",
            "  :/:\n",
            "  //example.com/NN\n",
            "  ,/,\n",
            "  but/CC\n",
            "  it/PRP\n",
            "  did/VBD\n",
            "  n't/RB\n",
            "  catch/VB\n",
            "  the/DT\n",
            "  fish/NN\n",
            "  because/IN\n",
            "  it/PRP\n",
            "  was/VBD\n",
            "  too/RB\n",
            "  quick/JJ\n",
            "  and/CC\n",
            "  the/DT\n",
            "  lazy/JJ\n",
            "  dog/NN\n",
            "  did/VBD\n",
            "  n't/RB\n",
            "  move/VB\n",
            "  quickly/RB\n",
            "  enough/RB\n",
            "  ./.)\n",
            "Lemmas: ['The', 'quick', 'brown', 'fox', 'is', 'jumping', 'over', 'the', 'lazy', 'dog', ',', 'which', 'is', 'located', 'at', 'http', ':', '//example.com', ',', 'but', 'it', 'did', \"n't\", 'catch', 'the', 'fish', 'because', 'it', 'wa', 'too', 'quick', 'and', 'the', 'lazy', 'dog', 'did', \"n't\", 'move', 'quickly', 'enough', '.']\n",
            "Filtered tokens: ['The', 'quick', 'brown', 'fox', 'jumping', 'lazy', 'dog', ',', 'located', 'http', ':', '//example.com', ',', \"n't\", 'catch', 'fish', 'quick', 'lazy', 'dog', \"n't\", 'move', 'quickly', 'enough', '.']\n",
            "Text without URLs: The quick brown fox is jumping over the lazy dog, which is located at , but it didn't catch the fish because it was too quick and the lazy dog didn't move quickly enough.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the classifier on a new example\n",
        "test_text = \"The movie was full of humors.\"\n",
        "print(\"Sentiment:\", classifier.classify(extract_features(test_text)))"
      ],
      "metadata": {
        "id": "6A0pX-LuNsYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20466857-b46b-4159-e69e-ead8a5c12ec1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the classifier on a new example\n",
        "test_text = \"I don't like the movie.\"\n",
        "print(\"Sentiment:\", classifier.classify(extract_features(test_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l90uQOgT4K0a",
        "outputId": "c7fb7dcf-89ef-45a4-f1b0-2481bf9c1684"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: neg\n"
          ]
        }
      ]
    }
  ]
}